{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hybrid Pipeline Demo: Vector Search + Knowledge Graph\n",
                "\n",
                "This notebook demonstrates the **2 Core Use Cases** for our Legal Knowledge Graph:\n",
                "\n",
                "1.  **Filtering**: Using the Graph to remove \"noise\" from Vector Search results.\n",
                "2.  **Context**: Using the Graph to find hidden connections (Context Enrichment).\n",
                "\n",
                "We will replicate a mini-version of the pipeline here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install gliner networkx matplotlib tqdm -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import NetworkX as nx\n",
                "import json\n",
                "from gliner import GLiNER\n",
                "import networkx as nx\n",
                "\n",
                "# 1. Setup - Let's build a tiny Graph from real data for the demo\n",
                "# We will use a small sample of bills to keep it fast.\n",
                "\n",
                "print(\"Initializing GLiNER...\")\n",
                "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
                "\n",
                "labels = [\"Person\", \"Legislator\", \"Committee\", \"Government Agency\", \"Bill\", \"Date\", \"Topic\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: Build a Mini-Graph (The Reference Knowledge)\n",
                "We ingest a few chunks. Imagine this is our **Master Knowledge Graph**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dummy Data (simulating content from bills.json)\n",
                "# We create 3 documents. \n",
                "# Doc 1 & 2 are about 'Senator Smith' and 'Energy'.\n",
                "# Doc 3 is about 'Energy' but completely unrelated to Smith (Noise).\n",
                "\n",
                "documents = [\n",
                "    {\n",
                "        \"id\": \"chunk_1\",\n",
                "        \"text\": \"Senator John Smith introduced the Clean Energy Act to reduce carbon emissions. He spoke before the Committee on Environment.\"\n",
                "    },\n",
                "    {\n",
                "        \"id\": \"chunk_2\",\n",
                "        \"text\": \"The Committee on Environment approved the budget proposed by Senator Smith for solar panel research.\"\n",
                "    },\n",
                "    {\n",
                "        \"id\": \"chunk_3\", \n",
                "        \"text\": \"The Department of Energy announced a new initiative for nuclear fusion. This has no relation to the Senate's recent activities.\"\n",
                "    }\n",
                "]\n",
                "\n",
                "print(\"Building Graph...\")\n",
                "G = nx.Graph()\n",
                "\n",
                "# Map to store check text for retrieval later\n",
                "chunk_store = {}\n",
                "\n",
                "for doc in documents:\n",
                "    chunk_id = doc['id']\n",
                "    text = doc['text']\n",
                "    chunk_store[chunk_id] = text\n",
                "    \n",
                "    # Add Chunk Node\n",
                "    G.add_node(chunk_id, type=\"Chunk\", text=text)\n",
                "    \n",
                "    # Extract Entities\n",
                "    entities = model.predict_entities(text, labels)\n",
                "    \n",
                "    for entity in entities:\n",
                "        name = entity['text'].strip()\n",
                "        label = entity['label']\n",
                "        \n",
                "        # Add Entity Node\n",
                "        G.add_node(name, type=label)\n",
                "        # Connect Chunk -> Entity\n",
                "        G.add_edge(chunk_id, name, relation=\"MENTIONS\")\n",
                "\n",
                "print(f\"Graph built with {G.number_of_nodes()} nodes.\")\n",
                "# Visualize specifically the connections for Chunk 1\n",
                "print(\"Chunk 1 connects to:\", list(G.neighbors(\"chunk_1\")))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Use Case 1 - Filtering (The \"Wide Net\" Refiner)\n",
                "**Scenario**: User searches for **\"What did Senator Smith do for Energy?\"**\n",
                "\n",
                "A typical Vector Search (searching for 'Energy') might return **Chunk 3** (Nuclear Fusion) because it scores high on 'Energy', even though Senator Smith isn't involved.\n",
                "\n",
                "We use the Graph to filter this out."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pipeline_search(query, vector_results_ids):\n",
                "    print(f\"--- User Query: '{query}' ---\")\n",
                "    \n",
                "    # 1. Extract Key Entities from Query using GLiNER\n",
                "    # We want to know WHO/WHAT matters in this query.\n",
                "    query_entities = model.predict_entities(query, labels)\n",
                "    key_entities = [e['text'].strip() for e in query_entities]\n",
                "    print(f\"Detected Query Entities: {key_entities}\")\n",
                "    \n",
                "    if not key_entities:\n",
                "        print(\"No specific entities found in query. Returning all vector results.\")\n",
                "        return vector_results_ids\n",
                "        \n",
                "    # 2. Filter Vector Results\n",
                "    filtered_results = []\n",
                "    \n",
                "    for chunk_id in vector_results_ids:\n",
                "        # Logic: Keep chunk IF connected to AT LEAST ONE key entity in the graph\n",
                "        # (Or a close neighbor, but we'll specific direct connection for demo)\n",
                "        \n",
                "        is_relevant = False\n",
                "        \n",
                "        # Get neighbors of this chunk in the graph\n",
                "        if chunk_id in G:\n",
                "            chunk_neighbors = list(G.neighbors(chunk_id))\n",
                "            \n",
                "            # Check intersection\n",
                "            full_matches = set(key_entities).intersection(set(chunk_neighbors))\n",
                "            \n",
                "            # Also check partial string matches (e.g. 'Smith' matches 'Senator John Smith')\n",
                "            partial_matches = []\n",
                "            for key_ent in key_entities:\n",
                "                for neighbor in chunk_neighbors:\n",
                "                    if key_ent in neighbor or neighbor in key_ent:\n",
                "                        partial_matches.append(neighbor)\n",
                "                        \n",
                "            if full_matches or partial_matches:\n",
                "                is_relevant = True\n",
                "                print(f\"  [KEEP] {chunk_id}: Linked to {partial_matches or full_matches}\")\n",
                "            else:\n",
                "                print(f\"  [DROP] {chunk_id}: No connection to {key_entities}\")\n",
                "        else:\n",
                "            print(f\"  [DROP] {chunk_id}: Not in graph\")\n",
                "            \n",
                "        if is_relevant:\n",
                "            filtered_results.append(chunk_id)\n",
                "            \n",
                "    return filtered_results\n",
                "\n",
                "# SIMULATION\n",
                "# Vector search (mock) returns ALL 3 chunks because they all mention 'Energy' or 'Smith'\n",
                "mock_vector_results = [\"chunk_1\", \"chunk_2\", \"chunk_3\"]\n",
                "\n",
                "user_query = \"What did Senator John Smith do about Energy?\"\n",
                "\n",
                "final_chunks = pipeline_search(user_query, mock_vector_results)\n",
                "print(f\"\\nFinal Results: {final_chunks} (Chunk 3 should be gone)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Use Case 2 - Context Enrichment (The \"Deepen\" Logic)\n",
                "**Scenario**: The user finds a chunk about the \"Clean Energy Act\".\n",
                "\n",
                "The text says \"The Act was introduced.\" It doesn't say *who* is on the committee reviewing it, or what *other* bills Smith sponsored. The Graph knows this.\n",
                "\n",
                "We use the Graph to inject this \"Missing Context\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_graph_context(chunk_id):\n",
                "    # 1. Identify Entities in the valid chunk\n",
                "    if chunk_id not in G: return \"\"\n",
                "    \n",
                "    entities_in_chunk = list(G.neighbors(chunk_id))\n",
                "    \n",
                "    context_facts = []\n",
                "    \n",
                "    # 2. Hop to 2nd degree neighbors\n",
                "    # e.g. Chunk -> Senator Smith -> Committee on Environment\n",
                "    # This tells us Smith is related to that Committee, even if the chunk didn't say \"Member of\"\n",
                "    \n",
                "    for entity in entities_in_chunk:\n",
                "        # Find what else this entity is connected to (other than the chunk itself)\n",
                "        related_nodes = list(G.neighbors(entity))\n",
                "        for related in related_nodes:\n",
                "            # Skip the chunk itself\n",
                "            if related.startswith(\"chunk_\"): \n",
                "                # Optional: If it connects to another Chunk, that's a \"Related Document\"!\n",
                "                if related != chunk_id:\n",
                "                    context_facts.append(f\"Related Document found: {related}\")\n",
                "                continue\n",
                "            \n",
                "            # Create a context sentence\n",
                "            # e.g. \"Senator John Smith\" is connected to \"Committee on Environment\"\n",
                "            context_facts.append(f\"{entity} is related to {related}\")\n",
                "            \n",
                "    return list(set(context_facts))\n",
                "\n",
                "# SIMULATION\n",
                "print(f\"--- enriching {final_chunks[0]} ---\")\n",
                "context = get_graph_context(final_chunks[0]) # Enrich Chunk 1\n",
                "\n",
                "print(f\"Original Text: \\n   '{chunk_store[final_chunks[0]]}'\\n\")\n",
                "print(\"Graph Context Added:\")\n",
                "for fact in context:\n",
                "    print(f\" + {fact}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion of Demo\n",
                "\n",
                "1.  **Filtering**: We successfully dropped `chunk_3`. Even though it matched the keyword \"Energy\", the Graph knew it wasn't connected to the primary query entity \"Senator John Smith\".\n",
                "2.  **Context**: We found that `Senator John Smith` is connected to `Committee on Environment` and `chunk_2`. This allows the LLM to answer questions like \"What other activities is Smith involved in?\" using the *structural* knowledge, not just the text."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}