{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Opinion Chunking\n",
                "\n",
                "This notebook loads the scraped opinions, cleans the text, and splits it into chunks while preserving metadata."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading data from opinions.json...\n",
                        "Loaded 100 opinions.\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import re\n",
                "import os\n",
                "from typing import List, Dict, Any\n",
                "\n",
                "INPUT_FILE = 'opinions.json'\n",
                "OUTPUT_FILE = 'opinion_chunks.json'\n",
                "\n",
                "print(f\"Loading data from {INPUT_FILE}...\")\n",
                "with open(INPUT_FILE, 'r') as f:\n",
                "    opinions = json.load(f)\n",
                "print(f\"Loaded {len(opinions)} opinions.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text: str) -> str:\n",
                "    if not text:\n",
                "        return \"\"\n",
                "    # Remove HTML tags\n",
                "    text = re.sub(r'<[^>]+>', '', text)\n",
                "    # Remove frequent \"Page\" headers or line numbers visible in some legal scraping\n",
                "    # (Matches simple \"Page X\" or numbers at start of lines)\n",
                "    text = re.sub(r'(?m)^\\s*\\d+\\s*$', '', text)\n",
                "    # Normalize whitespace\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text\n",
                "\n",
                "def recursive_split(text: str, max_chars: int = 1500, overlap: int = 100) -> List[str]:\n",
                "    \"\"\"\n",
                "    Splits text by sentences and groups them into chunks.\n",
                "    Only splits mid-sentence if a single sentence exceeds max_chars.\n",
                "    \"\"\"\n",
                "    # Simple regex for sentence splitting (handles common abbreviations roughly, but good enough for general legal text)\n",
                "    # Splits on period/question/exclamation followed by whitespace.\n",
                "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
                "    \n",
                "    chunks = []\n",
                "    current_chunk = \"\"\n",
                "    \n",
                "    for sentence in sentences:\n",
                "        sentence = sentence.strip()\n",
                "        if not sentence:\n",
                "            continue\n",
                "            \n",
                "        # Check if adding this sentence would exceed limit\n",
                "        if len(current_chunk) + len(sentence) + 1 <= max_chars:\n",
                "            current_chunk += (sentence + \" \")\n",
                "        else:\n",
                "            # Current chunk is full, save it\n",
                "            if current_chunk:\n",
                "                chunks.append(current_chunk.strip())\n",
                "            \n",
                "            # Now handle the new sentence\n",
                "            if len(sentence) > max_chars:\n",
                "                # If the sentence ITSELF is too big, we must hard split it\n",
                "                # We'll use a simple character split for this giant sentence\n",
                "                sub_chunks = [sentence[i:i+max_chars] for i in range(0, len(sentence), max_chars-overlap)]\n",
                "                chunks.extend(sub_chunks)\n",
                "                current_chunk = \"\" \n",
                "            else:\n",
                "                # Start a new chunk with this sentence\n",
                "                # Implement overlap: try to take the last sentence from previous chunk if possible, \n",
                "                # but for simple sentence grouping, clean breaks are often preferred.\n",
                "                # Here we just start fresh.\n",
                "                current_chunk = sentence + \" \"\n",
                "    \n",
                "    if current_chunk:\n",
                "        chunks.append(current_chunk.strip())\n",
                "        \n",
                "    return chunks\n",
                "\n",
                "def create_chunks(opinions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
                "    all_chunks = []\n",
                "    \n",
                "    for op in opinions:\n",
                "        # Prefer plain text, fallbacks handled here (though scraping usually fixes it)\n",
                "        raw_text = op.get('plain_text') or op.get('html') or op.get('html_lawbox') or \"\"\n",
                "        if not raw_text:\n",
                "            continue\n",
                "            \n",
                "        cleaned_text = clean_text(raw_text)\n",
                "        text_chunks = recursive_split(cleaned_text, max_chars=1500, overlap=100) # 1500 chars is roughly 250-300 words\n",
                "        \n",
                "        for i, chunk_text in enumerate(text_chunks):\n",
                "            chunk_data = op.copy()\n",
                "            # Remove the full text fields from the chunk to save space/confusion\n",
                "            for key in ['plain_text', 'html', 'html_lawbox', 'html_columbia', 'html_anon_2020', 'html_with_citations']:\n",
                "                chunk_data.pop(key, None)\n",
                "            \n",
                "            chunk_data['text'] = chunk_text\n",
                "            chunk_data['chunk_index'] = i\n",
                "            chunk_data['total_chunks'] = len(text_chunks)\n",
                "            all_chunks.append(chunk_data)\n",
                "            \n",
                "    return all_chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 670 chunks from 100 opinions.\n",
                        "Saved to opinion_chunks.json\n"
                    ]
                }
            ],
            "source": [
                "chunks = create_chunks(opinions)\n",
                "print(f\"Created {len(chunks)} chunks from {len(opinions)} opinions.\")\n",
                "\n",
                "with open(OUTPUT_FILE, 'w') as f:\n",
                "    json.dump(chunks, f, indent=2)\n",
                "    \n",
                "print(f\"Saved to {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample chunk:\n",
                        "{\n",
                        "  \"resource_uri\": \"https://www.courtlistener.com/api/rest/v3/opinions/11224238/\",\n",
                        "  \"id\": 11224238,\n",
                        "  \"absolute_url\": \"/opinion/10757653/prescila-lovell-for-herself-as-a-private-attorney-general-and-on-behalf/\",\n",
                        "  \"cluster_id\": 10757653,\n",
                        "  \"cluster\": \"https://www.courtlistener.com/api/rest/v3/clusters/10757653/\",\n",
                        "  \"author_id\": null,\n",
                        "  \"author\": null,\n",
                        "  \"joined_by\": [],\n",
                        "  \"date_created\": \"2025-12-13T11:56:49.962743-08:00\",\n",
                        "  \"date_modified\": \"2025-12-13T11:56:50.187131-08:00\",\n",
                        "  \"author_str\": \"\",\n",
                        "  \"per_curiam\": false,\n",
                        "  \"joined_by_str\": \"\",\n",
                        "  \"type\": \"100trialcourt\",\n",
                        "  \"sha1\": \"04ffbb08b99c24bd9f9805ab4a9676934db5fee8\",\n",
                        "  \"page_count\": 3,\n",
                        "  \"download_url\": null,\n",
                        "  \"local_path\": \"recap/gov.uscourts.caed.475980/gov.uscourts.caed.475980.16.0.pdf\",\n",
                        "  \"xml_harvard\": \"\",\n",
                        "  \"extracted_by_ocr\": true,\n",
                        "  \"ordering_key\": null,\n",
                        "  \"main_version\": null,\n",
                        "  \"opinions_cited\": [\n",
                        "    \"https://www.courtlistener.com/api/rest/v3/opinions/590987/\"\n",
                        "  ],\n",
                        "  \"text\": \"1 Stephanie Sheridan (CA 135910) Meegan B. Brooks (CA 298570) 2 Benesch, Friedlander, Coplan & Aronoff LLP 100 Pine Street, Suite 3100 3 San Francisco, California 94111 Telephone: 628.600.2250 4 Facsimile: 628.221.5828 Email: ssheridan@beneschlaw.com 5 mbrooks@beneschlaw.com Attorneys for Defendant 7 LOWE\\u2019S HOME CENTERS, LLC UNITED STATES DISTRICT COURT EASTERN DISTRICT OF CALIFORNIA SACRAMENTO DIVISION PRESCILA LOVELL, for herself, as a Case No. 2:25-cv-03453-TLN-SCR 13 private attorney general, and on behalf of all others similarly situated, 14 STIPULATION AND ORDER TO EXTEND Plaintiff, TIME FOR DEFENDANT LOWE\\u2019S HOME 15 CENTERS, LLC TO RESPOND TO v. PLAINTIFF\\u2019S COMPLAINT LOWE\\u2019S HOME CENTERS, LLC, Defendant. 1 Pursuant to Federal Rule of Civil Procedure 16(b)(4) and Local Rule 144(a), Plaintiff Prescila 2 Lovell (\\u201cPlaintiff\\u201d) and Defendant Lowe\\u2019s Home Centers, LLC (\\u201cDefendant\\u201d) (collectively, the 3 \\u201cParties\\u201d), by and through their respective counsel, hereby stipulate that Defendant\\u2019s deadline to respond 4 to the Complaint in the above-captioned action is extended a total of fifty-one (51) days from December 5 3, 2025 to and including January 23, 2026. A court may modify a deadline for good cause. Fed. R. Civ. 6 P. 6(b). Indeed, modifying or extending a responsive deadline is within the discretion of the trial judge. 7 See Johnson v. Mammoth Recreations, Inc., 975 F.2d 604, 609 (9th Cir.1992).\",\n",
                        "  \"chunk_index\": 0,\n",
                        "  \"total_chunks\": 3\n",
                        "}\n",
                        "\n",
                        "Text snippet: 1 Stephanie Sheridan (CA 135910) Meegan B. Brooks (CA 298570) 2 Benesch, Friedlander, Coplan & Aronoff LLP 100 Pine Street, Suite 3100 3 San Francisco, California 94111 Telephone: 628.600.2250 4 Facsi\n",
                        "Chunk ends with punctuation (likely sentence boundary preserved).\n"
                    ]
                }
            ],
            "source": [
                "# Verification\n",
                "print(f\"Sample chunk:\\n{json.dumps(chunks[0], indent=2)}\")\n",
                "assert 'text' in chunks[0]\n",
                "assert 'date_created' in chunks[0] # Verify metadata preservation\n",
                "\n",
                "# Check text cleaning quality (random check)\n",
                "sample_text = chunks[0]['text']\n",
                "print(\"\\nText snippet:\", sample_text[:200])\n",
                "assert '<' not in sample_text and '>' not in sample_text[:10], \"HTML tags might remain\"\n",
                "# Verify sentence ending if possible (rough heuristic)\n",
                "if len(sample_text) > 50 and sample_text[-1] not in ['.', '!', '?', '\"', \"'\"]:\n",
                "    print(\"Warning: Chunk might not end with a sentence punctuation (could be end of file or mid-sentence split).\")\n",
                "else:\n",
                "    print(\"Chunk ends with punctuation (likely sentence boundary preserved).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
