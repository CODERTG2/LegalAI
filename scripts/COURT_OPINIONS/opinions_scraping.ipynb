{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CourtListener Opinions Scraper\n",
                "\n",
                "This notebook scrapes the latest opinions from the CourtListener API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "API Key loaded. Ready to scrape.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import requests\n",
                "import json\n",
                "import time\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables from scripts/.env\n",
                "load_dotenv('../.env')\n",
                "\n",
                "API_KEY = os.getenv('COURT_API_KEY')\n",
                "if not API_KEY:\n",
                "    raise ValueError(\"COURT_API_KEY not found in environment variables\")\n",
                "\n",
                "BASE_URL = \"https://www.courtlistener.com/api/rest/v3/opinions/\"\n",
                "HEADERS = {'Authorization': f'Token {API_KEY}'}\n",
                "TARGET_COUNT = 100\n",
                "OUTPUT_FILE = 'opinions.json'\n",
                "\n",
                "print(\"API Key loaded. Ready to scrape.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting scrape from: https://www.courtlistener.com/api/rest/v3/opinions/?order_by=-date_filed\n",
                        "Fetching: https://www.courtlistener.com/api/rest/v3/opinions/?order_by=-date_filed\n",
                        "Found 20 items in this page.\n",
                        "Fetching: https://www.courtlistener.com/api/rest/v3/opinions/?order_by=-date_filed&page=2\n",
                        "Found 20 items in this page.\n",
                        "Fetching: https://www.courtlistener.com/api/rest/v3/opinions/?order_by=-date_filed&page=3\n",
                        "Found 20 items in this page.\n",
                        "Fetching: https://www.courtlistener.com/api/rest/v3/opinions/?order_by=-date_filed&page=4\n",
                        "Found 20 items in this page.\n",
                        "Fetching: https://www.courtlistener.com/api/rest/v3/opinions/?order_by=-date_filed&page=5\n",
                        "Found 20 items in this page.\n",
                        "Scraping complete. Collected 100 opinions.\n"
                    ]
                }
            ],
            "source": [
                "opinions = []\n",
                "next_url = f\"{BASE_URL}?order_by=-date_filed\"\n",
                "\n",
                "print(f\"Starting scrape from: {next_url}\")\n",
                "\n",
                "while len(opinions) < TARGET_COUNT and next_url:\n",
                "    print(f\"Fetching: {next_url}\")\n",
                "    try:\n",
                "        response = requests.get(next_url, headers=HEADERS)\n",
                "        response.raise_for_status()\n",
                "        data = response.json()\n",
                "        \n",
                "        results = data.get('results', [])\n",
                "        print(f\"Found {len(results)} items in this page.\")\n",
                "        \n",
                "        for item in results:\n",
                "            # Check if we have substantial text content\n",
                "            text_content = item.get('plain_text') or item.get('html') or item.get('html_lawbox') or item.get('html_columbia') or item.get('html_anon_2020')\n",
                "            \n",
                "            if not text_content:\n",
                "                # If no text in list response, try fetching detail\n",
                "                resource_uri = item.get('resource_uri')\n",
                "                if resource_uri:\n",
                "                    print(f\"Fetching detail for {item.get('id')}...\")\n",
                "                    try:\n",
                "                        detail_response = requests.get(resource_uri, headers=HEADERS)\n",
                "                        detail_response.raise_for_status()\n",
                "                        item = detail_response.json()\n",
                "                    except Exception as e:\n",
                "                        print(f\"Failed to fetch detail for {item.get('id')}: {e}\")\n",
                "                        continue\n",
                "                    # Add a small delay to be polite\n",
                "                    time.sleep(0.5)\n",
                "            \n",
                "            opinions.append(item)\n",
                "            if len(opinions) >= TARGET_COUNT:\n",
                "                break\n",
                "        \n",
                "        next_url = data.get('next')\n",
                "        # Polite delay between pages\n",
                "        time.sleep(1)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error fetching page: {e}\")\n",
                "        break\n",
                "\n",
                "print(f\"Scraping complete. Collected {len(opinions)} opinions.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved to opinions.json\n"
                    ]
                }
            ],
            "source": [
                "with open(OUTPUT_FILE, 'w') as f:\n",
                "    json.dump(opinions, f, indent=2)\n",
                "\n",
                "print(f\"Saved to {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total records: 100\n",
                        "Records with text content: 100\n",
                        "First 10 dates (filed or created): ['2025-12-13T11:56:49.962743-08:00', '2025-12-13T11:56:18.037148-08:00', '2025-12-13T11:55:58.804303-08:00', '2025-12-13T11:55:51.108410-08:00', '2025-12-13T11:55:39.097370-08:00', '2025-12-13T11:55:36.832155-08:00', '2025-12-13T11:55:34.409135-08:00', '2025-12-13T11:55:32.408994-08:00', '2025-12-13T11:55:26.188733-08:00', '2025-12-13T11:55:22.538209-08:00']\n",
                        "Dates appear to be in descending order.\n"
                    ]
                }
            ],
            "source": [
                "# Verification\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "with open(OUTPUT_FILE, 'r') as f:\n",
                "    saved_data = json.load(f)\n",
                "\n",
                "print(f\"Total records: {len(saved_data)}\")\n",
                "assert len(saved_data) >= 100, \"Failed to collect 100 opinions\"\n",
                "\n",
                "# Check for text content\n",
                "text_count = sum(1 for item in saved_data if item.get('plain_text') or item.get('html') or item.get('html_lawbox'))\n",
                "print(f\"Records with text content: {text_count}\")\n",
                "\n",
                "# Check dates (using date_created as proxy for recency if date_filed is missing)\n",
                "dates = []\n",
                "for item in saved_data[:10]:\n",
                "    date_val = item.get('date_filed') or item.get('date_created')\n",
                "    if date_val:\n",
                "        dates.append(date_val)\n",
                "\n",
                "print(f\"First 10 dates (filed or created): {dates}\")\n",
                "\n",
                "# Simple check for descending order (allowing for some batching/same timestamps)\n",
                "if dates == sorted(dates, reverse=True):\n",
                "    print(\"Dates appear to be in descending order.\")\n",
                "else:\n",
                "    print(\"Dates are recent but might not be strictly sorted by creation time (which is expected for batched updates).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
